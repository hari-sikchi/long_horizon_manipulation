{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from maddux.rl_experiments.throwing import ThrowingArm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpruAUgJ\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = tempfile.mkdtemp()\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = ThrowingArm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow business - it is always good to reset a graph before creating a new controller.\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This little guy will let us run tensorboard\n",
    "#      tensorboard --logdir [LOG_DIR]\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "brain = MLP([game.observation_size,], [200, 200, game.num_actions], \n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(game.observation_size, game.num_actions, brain, \n",
    "                                   optimizer, session, discount_rate=0.90, \n",
    "                                   exploration_period=5000, max_experience=10000, \n",
    "                                   store_every_nth=4, train_every_nth=4,\n",
    "                                   summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "# graph was not available when journalist was created  \n",
    "journalist.add_graph(session.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Game 0: iterations before success 20. Distance to target: 8.28926377533 Last 5 rewards: [-1.3486394134307744, -1.5972081466433252, -1.8647494063888104, 0.10602198887177394, -8.2892637753344918]"
     ]
    }
   ],
   "source": [
    "performances = []\n",
    "\n",
    "try:\n",
    "    for game_idx in range(10000):\n",
    "        game = ThrowingArm()\n",
    "        game_iterations = 0\n",
    "\n",
    "        observation = game.observe()\n",
    "        while game_iterations < 1000 and not game.is_over():\n",
    "            action = current_controller.action(observation)\n",
    "            reward = game.collect_reward(action)\n",
    "            new_observation = game.observe()\n",
    "            current_controller.store(observation, action, reward, new_observation)\n",
    "            current_controller.training_step()\n",
    "            observation = new_observation\n",
    "            game_iterations += 1\n",
    "        performance = float(game_iterations - (game.distance_to_target)) / game.distance_to_target\n",
    "        performances.append(performance)\n",
    "        if game_idx % 50 == 0:\n",
    "            print \"\\rGame %d: iterations before success %d.\" % (game_idx, game_iterations),\n",
    "            print \"Distance to target: %s\" % (game.distance_to_target),\n",
    "            print \"Last 5 rewards: {}\".format(game.collected_rewards[-5:]),\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print \"Interrupted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.494897427831781"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
